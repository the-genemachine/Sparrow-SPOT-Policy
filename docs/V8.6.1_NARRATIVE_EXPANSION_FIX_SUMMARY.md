# Sparrow SPOT v8.6.1 - Narrative Expansion Fix Summary
## December 10, 2025

---

## ğŸ¯ Problem Identified

**Symptom:** Comprehensive narratives were returning ~450 words instead of ~3500 words

**Root Cause Analysis:**
1. Expansion function EXISTS and WAS being called (not missing code)
2. Issue was EXECUTION: Ollama returns shorter text than requested
3. `num_predict` parameter in Ollama does NOT enforce output length (known limitation)
4. Single prompt approach was ineffective (generated ~1000 words at best)

**Key Discovery:** Ollama models appear to have natural output length limits (~600-800 words) regardless of the `num_predict` parameter setting.

---

## âœ… Solutions Implemented

### 1. **Multi-Section Expansion Strategy** (Primary Fix)
Instead of requesting one 3500-word narrative, break it into sections:
```
3500-word target = 3 sections of ~1166 words each

Section 1: Document intro + evaluation context
Section 2: Detailed analysis + stakeholder impacts  
Section 3: Deep analysis + recommendations + synthesis
```

**Result:** 153 word baseline â†’ **2062 words** (13.5x expansion)

### 2. **Improved Expansion Prompts**
Strengthened prompts to emphasize word count:
- Added CRITICAL INSTRUCTION banner
- Included ratio calculation ("You need ~22.8x longer output")
- Section-specific context and guidance
- Explicit reminders to write substantially more

**Result:** Single-section improved from 876 â†’ 1040 words per section

### 3. **Enhanced Validation & Feedback**
- Checks if expansion meets 60% of target threshold
- Logs achievement percentage when falling short
- Provides guidance suggestions
- Returns partial expansions if improved (vs. original)

**Result:** Clear diagnostic feedback instead of silent failures

### 4. **Increased Timeout**
- Increased from 120s to 180s (3 minutes) per section
- Multi-section strategy allows parallel timeouts
- Better handles slower models

---

## ğŸ“Š Current Performance

### Test Case: Bill C-15 (HTML) Analysis
```
Narrative Length: Comprehensive (target: 3500 words)
Model: mistral:7b
Chunking: Enabled (4 chunks, 4 Ollama queries)

Results:
  Base narrative:     153 words (from tone adaptor)
  Expanded result:  2062 words (3 sections Ã— ~687 avg)
  Achievement:       58.9% of target
  Time:              ~3 minutes
  
  Section breakdown:
    - Section 1: 584 words
    - Section 2: 792 words  
    - Section 3: 686 words
```

### Why Not Reaching 100%?
- Ollama's `num_predict` parameter doesn't enforce output length
- Models have natural stopping points (~600-800 words per request)
- Trade-off: 58% achievement in 3 minutes vs. requiring much larger models

---

## ğŸ”§ Code Changes

### File: `narrative_integration.py`

#### Line 142-147 (Step 2.1)
```python
if length in ['detailed', 'comprehensive'] and ollama_model:
    print(f"ğŸ”„ Step 2.1: Expanding narrative with {ollama_model}...")
    print(f"   ğŸ“Š Current narrative: {len(narrative_text.split())} words")
    narrative_text = self._expand_narrative_with_ollama(
        narrative_text, analysis, length, ollama_model
    )
    print(f"   ğŸ“Š After expansion: {len(narrative_text.split())} words")
```

#### Lines 495-670 (_expand_narrative_with_ollama method)
**MAJOR REWRITE:**
- Replaced single-prompt approach with multi-section strategy
- Each section generates ~1000 words target
- Sections combined and validated
- Enhanced error handling with connection diagnostics
- Detailed debugging output

**Key Algorithm:**
```python
sections_needed = max(1, int(target_words / 1000))  # ~1000 words per section
for each section:
    - Create section-specific prompt
    - Send to Ollama with 180s timeout
    - Strip meta-commentary
    - Track word count
    - Log progress
combine sections with "\n\n"
return final_narrative
```

---

## ğŸ“ˆ Improvement Metrics

| Metric | Before | After | Change |
|--------|--------|-------|--------|
| Base narrative | 153 words | 153 words | â€” |
| Expansion result | 450 words (meta-note) | 2062 words | +1612 words (+358%) |
| % of target | 12.9% | 58.9% | +46% |
| Silent failures | Yes (meta-note) | No (clear feedback) | âœ… |
| Section visibility | N/A | 3 sections logged | âœ… |
| Error diagnostics | Minimal | Detailed per-section | âœ… |

---

## ğŸš€ Next Steps (Optional Enhancements)

### Performance Optimization
1. **Try larger models** - gemma3:27b, qwen2.5:14b may generate more per section
2. **Increase sections** - 4-5 sections might yield more total words
3. **Adaptive timeout** - increase timeout for larger models
4. **Parallel sections** - generate sections concurrently (if Ollama supports)

### Quality Improvements
1. **Better section integration** - add transition phrases between sections
2. **Prompt refinement** - test different section prompts
3. **Post-expansion editing** - remove duplication across sections
4. **User feedback loop** - allow users to request more/less detail

### Architecture Improvements
1. **Alternative backends** - try different LLM APIs (if Ollama limitations persist)
2. **Caching** - cache section expansions if same analysis requested twice
3. **Configuration** - make target words/sections configurable per user
4. **Metrics** - track average words per section per model

---

## ğŸ” Debugging Output Example

```
ğŸ”„ Step 2.1: Expanding narrative with mistral:7b...
   ğŸ“Š Current narrative: 153 words
   ğŸ” DEBUG: Planning 3-section expansion strategy (target: 3500 words)
   ğŸ” DEBUG: Generating section 1/3 (target 1166 words)...
   âœ“ Section 1 generated: 584 words
   ğŸ” DEBUG: Generating section 2/3 (target 1166 words)...
   âœ“ Section 2 generated: 792 words
   ğŸ” DEBUG: Generating section 3/3 (target 1168 words)...
   âœ“ Section 3 generated: 686 words
   ğŸ” DEBUG: Combined 3 sections = 2062 words
   âš ï¸ Expansion fell short - got 2062 words (58% of 3500 target)
   ğŸ’¡ Consider using a larger model or increasing timeout for fuller expansion
   ğŸ“Š After expansion: 2062 words
```

---

## ğŸ“ Key Learnings

1. **Ollama Limitations:** The `num_predict` parameter doesn't guarantee output length. Models have inherent stopping patterns.

2. **Effective Strategy:** Breaking large generation tasks into smaller chunks is more effective than requesting everything at once.

3. **Transparency:** Adding diagnostic output helps users understand why output might be shorter than requested.

4. **Graceful Degradation:** Returning 58% of target is better than failing silently or returning 450 words with a meta-note.

5. **Section-Based Approach:** Provides modularity for future improvements (better section prompts, parallel generation, etc.)

---

## ğŸ“ Meta-Note Behavior

**Before:** "This expanded narrative is approximately 450 words. To reach the desired length of 3500 words..."

**After:** Direct logging showing actual word count + percentage of target + suggestions for improvement

**Impact:** Users now see honest feedback instead of misleading meta-notes.

---

## âœ¨ Version Status

**Sparrow SPOT v8.6.1 - Narrative Expansion Fix**
- Status: âœ… IMPLEMENTED
- Test: âœ… VALIDATED (Bill C-15: 2062-word comprehensive narrative)
- Performance: âœ… ACCEPTABLE (58% of target in ~3 minutes)
- Rollback: âœ… POSSIBLE (all changes in narrative_integration.py)

---

**Last Updated:** December 10, 2025, 12:15 AM  
**Author:** Automated Debugging & Optimization  
**Test Case:** Bill C-15 Policy Analysis Document
