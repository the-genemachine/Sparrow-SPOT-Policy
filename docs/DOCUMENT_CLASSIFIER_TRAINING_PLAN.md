# Document-Type AI Classifier Training Plan

**Project:** Sparrow SPOT Scale™ Domain-Specific AI Detection  
**Version:** Proposed v9.0 Enhancement  
**Date:** December 3, 2025  
**Status:** Planning Phase

---

## Executive Summary

Current AI detection methods are trained on generic text and produce high false-positive rates on specialized documents. This plan outlines the development of **domain-specific AI classifiers** trained to distinguish human-authored vs AI-generated content within specific document types.

### Problem Statement

| Document Type | Current False Positive Rate | Root Cause |
|---------------|---------------------------|------------|
| Legislation | ~40-50% | Formulaic drafting conventions |
| Budget Documents | ~35-45% | Structured fiscal language |
| Legal Judgments | ~30-40% | Citation patterns, legal reasoning |
| Policy Briefs | ~25-35% | Executive summary format |
| Research Reports | ~20-30% | Academic structure, hedging |
| News Articles | ~15-25% | AP style, attribution patterns |

### Proposed Solution

Train **6-8 specialized classifiers**, one per document type, using:
1. **Baseline corpus**: Known human-authored documents (pre-2020)
2. **AI-generated corpus**: Same document types generated by modern LLMs
3. **Fine-tuned discriminator**: Small transformer model per document type

---

## Phase 1: Data Collection (Weeks 1-3)

### 1.1 Canadian Legislation Corpus

**Target**: 2,000+ documents

| Source | Type | Estimated Count | Time Period |
|--------|------|-----------------|-------------|
| LEGISinfo | Federal Bills | 500+ | 2000-2019 |
| Justice Laws | Consolidated Acts | 300+ | Pre-2020 versions |
| Provincial Legislatures | Provincial Bills | 500+ | 2010-2019 |
| CanLII | Regulations | 400+ | Pre-2020 |
| UK Parliament | Westminster Bills | 300+ | For comparison |

**Collection Method**:
```python
# Automated scraping pipeline
sources = {
    'legisinfo': 'https://www.parl.ca/legisinfo/',
    'justice_laws': 'https://laws-lois.justice.gc.ca/',
    'canlii': 'https://www.canlii.org/',
}
```

**Key Characteristics to Preserve**:
- Enumeration patterns (a), (b), (c)
- Section/subsection structure
- Amendment language
- Bilingual formatting (EN/FR)
- Royal Assent markers

### 1.2 Budget Documents Corpus

**Target**: 1,000+ documents

| Source | Type | Estimated Count |
|--------|------|-----------------|
| Finance Canada | Federal Budgets | 50+ (1995-2019) |
| Treasury Board | Main Estimates | 100+ |
| Departmental Plans | Annual Reports | 300+ |
| Provincial Budgets | All provinces | 200+ |
| PBO Reports | Fiscal Analysis | 150+ |
| Auditor General | Annual Reports | 100+ |

**Key Characteristics**:
- Vote structures
- Appropriation language
- Fiscal year references
- Table formatting
- Percentage expressions
- Year-over-year comparisons

### 1.3 Legal Judgments Corpus

**Target**: 3,000+ documents

| Source | Type | Estimated Count |
|--------|------|-----------------|
| CanLII | SCC Decisions | 500+ |
| CanLII | Federal Court | 1,000+ |
| CanLII | Provincial Courts | 1,000+ |
| UK BAILII | Commonwealth comparison | 500+ |

**Key Characteristics**:
- Paragraph numbering [1], [2], [3]
- Case citations (neutral citations)
- Reasons for judgment structure
- Latin legal terms
- Stare decisis references

### 1.4 Policy Briefs Corpus

**Target**: 1,500+ documents

| Source | Type | Estimated Count |
|--------|------|-----------------|
| IRPP | Policy papers | 200+ |
| C.D. Howe | Research papers | 300+ |
| Fraser Institute | Policy studies | 200+ |
| Conference Board | Briefings | 200+ |
| Government white papers | Policy proposals | 300+ |
| Think tanks (international) | Comparison set | 300+ |

**Key Characteristics**:
- Executive summary format
- Recommendation numbering
- Stakeholder language
- Options analysis structure
- Implementation timelines

### 1.5 Research Reports Corpus

**Target**: 2,000+ documents

| Source | Type | Estimated Count |
|--------|------|-----------------|
| Statistics Canada | Research papers | 500+ |
| CIHI | Health research | 300+ |
| Universities | Institutional reports | 500+ |
| NSERC/SSHRC | Funded research | 400+ |
| International (OECD, IMF) | Comparison | 300+ |

**Key Characteristics**:
- IMRaD structure (Introduction, Methods, Results, Discussion)
- Citation formats (APA, Chicago)
- Statistical language
- Limitation acknowledgments
- Peer review markers

### 1.6 News Articles Corpus

**Target**: 5,000+ documents

| Source | Type | Estimated Count |
|--------|------|-----------------|
| Canadian Press archives | Wire stories | 2,000+ |
| Globe and Mail (pre-2020) | National news | 1,000+ |
| CBC Archives | Broadcast transcripts | 1,000+ |
| Local newspapers | Regional coverage | 1,000+ |

**Key Characteristics**:
- Inverted pyramid structure
- Dateline formatting
- Attribution patterns ("said", "according to")
- AP/CP style conventions
- Quote integration

---

## Phase 2: AI Content Generation (Weeks 3-4)

### 2.1 Generation Strategy

For each human document in the corpus, generate **AI equivalents** using multiple models:

```python
GENERATION_MODELS = [
    'gpt-4-turbo',      # OpenAI flagship
    'claude-3-opus',    # Anthropic flagship
    'gemini-pro',       # Google
    'llama-3-70b',      # Meta (via Ollama)
    'mistral-large',    # Mistral AI
    'command-r-plus',   # Cohere
]
```

### 2.2 Generation Prompts by Document Type

**Legislation**:
```
You are a Canadian legislative drafter. Write a bill that would:
[Topic extracted from original bill]

Follow the format of Canadian federal legislation including:
- Short title and long title
- Preamble (if applicable)
- Definitions section
- Numbered sections and subsections
- Coming into force provisions

The bill should be approximately [X] sections long.
```

**Budget Documents**:
```
You are a Treasury Board analyst. Prepare departmental estimates for:
[Department extracted from original]

Include:
- Vote structure with appropriations
- Program activity descriptions
- Year-over-year comparisons
- Performance indicators
- Financial tables

Follow Government of Canada estimates format.
```

**Legal Judgments**:
```
You are a [Federal Court/SCC] judge. Write reasons for judgment in:
[Case type and parties]

Include:
- Numbered paragraphs
- Statement of facts
- Issues to be determined
- Analysis with case citations
- Conclusion and disposition

Follow Canadian judicial writing conventions.
```

### 2.3 Quality Control

Each generated document must:
1. Match approximate length of original (±20%)
2. Cover same topic/subject matter
3. Follow correct document structure
4. Pass basic format validation

**Rejection Criteria**:
- Anachronistic references (post-2020 events)
- Wrong jurisdiction markers
- Incomplete structure
- Obvious AI artifacts (emojis, markdown in formal docs)

---

## Phase 3: Model Training (Weeks 5-7)

### 3.1 Model Architecture

**Base Model**: DeBERTa-v3-large (304M parameters)
- State-of-the-art for text classification
- Handles long documents well
- Efficient fine-tuning with LoRA

**Alternative Options**:
| Model | Parameters | Pros | Cons |
|-------|-----------|------|------|
| DeBERTa-v3-large | 304M | Best accuracy | Slower inference |
| RoBERTa-large | 355M | Well-tested | Older architecture |
| Longformer | 149M | Long context | Less accurate |
| DistilBERT | 66M | Fast | Lower accuracy |

### 3.2 Training Configuration

```python
TRAINING_CONFIG = {
    'base_model': 'microsoft/deberta-v3-large',
    'max_length': 2048,  # tokens
    'batch_size': 8,
    'learning_rate': 2e-5,
    'epochs': 5,
    'warmup_ratio': 0.1,
    'weight_decay': 0.01,
    'lora_rank': 16,
    'lora_alpha': 32,
    'target_modules': ['query_proj', 'value_proj'],
}
```

### 3.3 Training Data Split

For each document type:

| Split | Percentage | Purpose |
|-------|-----------|---------|
| Training | 70% | Model learning |
| Validation | 15% | Hyperparameter tuning |
| Test | 15% | Final evaluation |

**Stratification**: Equal human/AI split in each partition

### 3.4 Training Pipeline

```python
from transformers import AutoModelForSequenceClassification, Trainer
from peft import get_peft_model, LoraConfig

def train_document_classifier(doc_type: str):
    """Train classifier for specific document type."""
    
    # Load base model
    model = AutoModelForSequenceClassification.from_pretrained(
        'microsoft/deberta-v3-large',
        num_labels=2,  # human vs AI
    )
    
    # Apply LoRA for efficient fine-tuning
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=['query_proj', 'value_proj'],
        lora_dropout=0.1,
    )
    model = get_peft_model(model, lora_config)
    
    # Load document-specific dataset
    train_dataset = load_dataset(f'spot/{doc_type}_train')
    eval_dataset = load_dataset(f'spot/{doc_type}_eval')
    
    # Train
    trainer = Trainer(
        model=model,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        # ... training args
    )
    trainer.train()
    
    # Save document-specific adapter
    model.save_pretrained(f'models/{doc_type}_classifier')
```

### 3.5 Per-Document-Type Models

| Document Type | Model Name | Training Size | Expected Accuracy |
|---------------|-----------|---------------|-------------------|
| Legislation | `spot-legislation-v1` | 4,000 docs | 92-95% |
| Budget | `spot-budget-v1` | 2,000 docs | 90-93% |
| Legal Judgment | `spot-legal-v1` | 6,000 docs | 91-94% |
| Policy Brief | `spot-policy-v1` | 3,000 docs | 88-92% |
| Research Report | `spot-research-v1` | 4,000 docs | 89-92% |
| News Article | `spot-news-v1` | 10,000 docs | 93-96% |

---

## Phase 4: Evaluation & Validation (Weeks 7-8)

### 4.1 Metrics

| Metric | Target | Description |
|--------|--------|-------------|
| Accuracy | >90% | Overall correct classification |
| Precision (AI) | >92% | When we say AI, we're right |
| Recall (AI) | >88% | We catch most AI content |
| F1 Score | >90% | Balanced performance |
| False Positive Rate | <8% | Human wrongly flagged as AI |

### 4.2 Validation Protocol

**Holdout Test Set**: Never seen during training
```python
def evaluate_classifier(model, test_set):
    predictions = model.predict(test_set)
    
    metrics = {
        'accuracy': accuracy_score(test_set.labels, predictions),
        'precision': precision_score(test_set.labels, predictions),
        'recall': recall_score(test_set.labels, predictions),
        'f1': f1_score(test_set.labels, predictions),
        'confusion_matrix': confusion_matrix(test_set.labels, predictions),
    }
    return metrics
```

**Cross-Validation**: 5-fold for robustness
**Adversarial Testing**: AI content edited by humans

### 4.3 Baseline Comparison

Compare trained models against:
1. Current SPOT v8.3.4 heuristics
2. GPTZero API
3. Copyleaks API
4. Zero-shot LLM classification

---

## Phase 5: Integration (Weeks 8-9)

### 5.1 Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    SPOT Scale v9.0                       │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  ┌──────────────────┐    ┌─────────────────────────┐   │
│  │ Document Type    │───▶│ Model Router            │   │
│  │ Detector         │    │                         │   │
│  └──────────────────┘    │ legislation → spot-leg  │   │
│                          │ budget → spot-budget    │   │
│                          │ legal → spot-legal      │   │
│                          │ policy → spot-policy    │   │
│                          │ research → spot-research│   │
│                          │ news → spot-news        │   │
│                          │ unknown → ensemble      │   │
│                          └─────────────────────────┘   │
│                                     │                   │
│                                     ▼                   │
│                          ┌─────────────────────────┐   │
│                          │ Trained Classifier      │   │
│                          │ (LoRA adapter loaded)   │   │
│                          └─────────────────────────┘   │
│                                     │                   │
│                                     ▼                   │
│                          ┌─────────────────────────┐   │
│                          │ Confidence Calibration  │   │
│                          │ + Explanation Generator │   │
│                          └─────────────────────────┘   │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

### 5.2 New Module: `trained_classifier.py`

```python
"""
Trained Document-Type AI Classifier for SPOT Scale v9.0

Uses fine-tuned DeBERTa models for accurate, domain-specific
AI content detection.
"""

from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import PeftModel
import torch

class TrainedDocumentClassifier:
    """Load and run trained classifiers for each document type."""
    
    MODELS = {
        'legislation': 'models/spot-legislation-v1',
        'budget': 'models/spot-budget-v1',
        'legal_judgment': 'models/spot-legal-v1',
        'policy_brief': 'models/spot-policy-v1',
        'research_report': 'models/spot-research-v1',
        'news_article': 'models/spot-news-v1',
    }
    
    def __init__(self):
        self.base_model = None
        self.tokenizer = None
        self.loaded_adapters = {}
        
    def load_base_model(self):
        """Load base DeBERTa model (shared across all adapters)."""
        self.base_model = AutoModelForSequenceClassification.from_pretrained(
            'microsoft/deberta-v3-large',
            num_labels=2,
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            'microsoft/deberta-v3-large'
        )
        
    def classify(self, text: str, document_type: str) -> dict:
        """
        Classify text as human or AI-generated.
        
        Returns:
            {
                'ai_probability': 0.0-1.0,
                'human_probability': 0.0-1.0,
                'classification': 'human' | 'ai' | 'uncertain',
                'confidence': 0.0-1.0,
                'model_used': str,
            }
        """
        if document_type not in self.MODELS:
            document_type = 'news_article'  # fallback
            
        # Load adapter if not cached
        if document_type not in self.loaded_adapters:
            adapter_path = self.MODELS[document_type]
            model = PeftModel.from_pretrained(self.base_model, adapter_path)
            self.loaded_adapters[document_type] = model
            
        model = self.loaded_adapters[document_type]
        
        # Tokenize
        inputs = self.tokenizer(
            text,
            max_length=2048,
            truncation=True,
            return_tensors='pt',
        )
        
        # Predict
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)
            
        human_prob = probs[0][0].item()
        ai_prob = probs[0][1].item()
        
        # Classification with uncertainty band
        if ai_prob > 0.7:
            classification = 'ai'
        elif ai_prob < 0.3:
            classification = 'human'
        else:
            classification = 'uncertain'
            
        return {
            'ai_probability': ai_prob,
            'human_probability': human_prob,
            'classification': classification,
            'confidence': abs(ai_prob - 0.5) * 2,  # 0-1 scale
            'model_used': f'spot-{document_type}-v1',
        }
```

### 5.3 Integration with Existing Detection

```python
# In ai_detection_engine.py v9.0

class AIDetectionEngine:
    def __init__(self):
        # Existing heuristic detectors
        self.heuristic_detectors = [...]
        
        # NEW: Trained classifiers
        self.trained_classifier = None
        if TRAINED_MODELS_AVAILABLE:
            self.trained_classifier = TrainedDocumentClassifier()
            self.trained_classifier.load_base_model()
    
    def analyze_document(self, text: str, document_type: str = None):
        # 1. Detect document type
        detected_type = self.detect_document_type(text, document_type)
        
        # 2. Run trained classifier (if available)
        if self.trained_classifier:
            trained_result = self.trained_classifier.classify(text, detected_type)
            
            # Trained model is PRIMARY, heuristics are SECONDARY
            if trained_result['confidence'] > 0.6:
                return self._build_result_from_trained(trained_result)
        
        # 3. Fallback to heuristic detection
        return self._heuristic_detection(text, detected_type)
```

---

## Phase 6: Deployment (Week 9-10)

### 6.1 Model Distribution

**Option A: Bundled with SPOT**
```
Sparrow-SPOT-Policy/
├── models/
│   ├── spot-legislation-v1/
│   │   ├── adapter_config.json
│   │   └── adapter_model.bin  (~50MB)
│   ├── spot-budget-v1/
│   ├── spot-legal-v1/
│   └── ...
```
- **Total size**: ~300MB for all adapters
- **Pros**: Works offline, no API costs
- **Cons**: Larger repo, requires GPU for best performance

**Option B: Hugging Face Hub**
```python
# Download on first use
model = PeftModel.from_pretrained(
    base_model,
    'spot-policy/legislation-classifier-v1',
)
```
- **Pros**: Smaller repo, easy updates
- **Cons**: Requires internet, download time

**Option C: Hybrid**
- Ship lightweight DistilBERT versions bundled
- Download full DeBERTa models on demand

### 6.2 Hardware Requirements

| Configuration | Min RAM | GPU | Inference Speed |
|--------------|---------|-----|-----------------|
| CPU only | 8GB | None | ~5 sec/doc |
| GPU (RTX 3060) | 8GB | 12GB VRAM | ~0.3 sec/doc |
| GPU (RTX 4090) | 16GB | 24GB VRAM | ~0.1 sec/doc |
| Apple Silicon M2 | 16GB | Unified | ~0.5 sec/doc |

### 6.3 Fallback Strategy

```python
def get_classifier():
    """Get best available classifier."""
    
    # Try full trained model
    if torch.cuda.is_available() and FULL_MODELS_AVAILABLE:
        return TrainedDocumentClassifier('deberta-large')
    
    # Try CPU-optimized model
    if DISTIL_MODELS_AVAILABLE:
        return TrainedDocumentClassifier('distilbert')
    
    # Fall back to heuristics
    return HeuristicClassifier()  # Current v8.3.4 approach
```

---

## Resource Requirements

### Personnel

| Role | Effort | Responsibility |
|------|--------|----------------|
| ML Engineer | 4 weeks | Model training, evaluation |
| Data Engineer | 2 weeks | Corpus collection, preprocessing |
| Backend Developer | 2 weeks | Integration, API |
| Domain Expert | 1 week | Validation, edge cases |

### Compute

| Phase | Resources | Estimated Cost |
|-------|-----------|----------------|
| Data collection | CPU instances | ~$100 |
| AI generation | GPT-4 API | ~$500 |
| Model training | A100 GPU (40GB) x 7 days | ~$1,500 |
| Evaluation | CPU instances | ~$100 |
| **Total** | | **~$2,200** |

### Timeline

```
Week 1-2:  ████████░░░░░░░░░░░░  Data collection (legislation, budget)
Week 2-3:  ░░░░████████░░░░░░░░  Data collection (legal, policy, news)
Week 3-4:  ░░░░░░░░████████░░░░  AI content generation
Week 5-6:  ░░░░░░░░░░░░████████  Model training
Week 7:    ░░░░░░░░░░░░░░░░████  Evaluation
Week 8-9:  ░░░░░░░░░░░░░░░░░░██  Integration
Week 10:   ░░░░░░░░░░░░░░░░░░░█  Deployment

Total: 10 weeks
```

---

## Success Criteria

### Minimum Viable Product (MVP)

- [ ] 3+ document type classifiers trained (legislation, budget, legal)
- [ ] >90% accuracy on holdout test sets
- [ ] <10% false positive rate on human-authored documents
- [ ] Integration with SPOT Scale GUI
- [ ] Fallback to heuristics when models unavailable

### Full Release

- [ ] All 6 document type classifiers trained
- [ ] >92% accuracy across all types
- [ ] <5% false positive rate
- [ ] Confidence calibration implemented
- [ ] Explanation generation for classifications
- [ ] Model versioning and update mechanism

---

## Risk Mitigation

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Insufficient training data | Medium | High | Partner with libraries, use synthetic augmentation |
| Model overfits to format | Medium | Medium | Cross-domain validation, adversarial testing |
| GPU costs exceed budget | Low | Medium | Use smaller models, LoRA efficiency |
| Models don't generalize | Medium | High | Diverse training data, ensemble methods |
| Integration complexity | Low | Low | Modular design, clear interfaces |

---

## Future Enhancements (v9.1+)

1. **Continuous Learning**: Update models as new AI systems emerge
2. **Multi-Model Attribution**: Not just "AI" but "which AI model"
3. **Paragraph-Level Detection**: Identify specific AI-written sections
4. **Confidence Explanation**: Why the model made its decision
5. **Adversarial Robustness**: Resist attempts to fool the detector
6. **Multilingual Support**: French, Indigenous languages

---

## Appendix A: Data Collection Scripts

```python
# scripts/collect_legislation.py

import requests
from bs4 import BeautifulSoup
import json
from pathlib import Path

def collect_legisinfo_bills(start_parliament=36, end_parliament=43):
    """Collect federal bills from LEGISinfo (pre-2020)."""
    
    base_url = "https://www.parl.ca/legisinfo/en/bills"
    bills = []
    
    for parliament in range(start_parliament, end_parliament + 1):
        # Scrape bill list
        # Download full text
        # Extract and clean
        pass
    
    return bills

def collect_justice_laws():
    """Collect consolidated acts from Justice Laws website."""
    pass

def collect_canlii_judgments():
    """Collect court decisions from CanLII."""
    pass
```

---

## Appendix B: Model Cards

### spot-legislation-v1

| Property | Value |
|----------|-------|
| Base Model | microsoft/deberta-v3-large |
| Training Data | 4,000 Canadian legislative documents |
| Labels | human, ai |
| Accuracy | TBD |
| Limitations | Optimized for Canadian federal/provincial legislation |

*(Similar cards for each document type)*

---

## Approval

| Role | Name | Date | Signature |
|------|------|------|-----------|
| Project Lead | | | |
| Technical Lead | | | |
| Domain Expert | | | |

---

*Document Classifier Training Plan — Sparrow SPOT Scale™ v9.0*
