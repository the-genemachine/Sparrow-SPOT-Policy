# Sparrow SPOT Scaleâ„¢ v8.4.0 - Discrepancy Resolution Todo

**Date:** December 4, 2025  
**Source:** Bill C-15-12 Discrepancy Report Analysis  
**Status:** Pending Review

---

## Executive Summary

This document outlines 12 systematic fixes required to resolve recurring discrepancies identified in the Bill C-15-12 assessment. These issues represent fundamental logic conflicts that undermine the credibility of analysis outputs.

---

## ðŸ”´ IMMEDIATE PRIORITY (Critical)

### 1. Implement INCONCLUSIVE Detection Reporting

**Problem:** Detection methods produce wildly divergent results (5%-90% range, 85-point spread) but system reports averaged scores as if they were reliable.

**Solution:**
- When detection spread >50%, flag analysis as `INCONCLUSIVE`
- Suppress detailed attribution and section analysis
- Primary message: *"Detection methods disagree - manual review required"*

**Files to Modify:**
- `ai_detection_engine.py`
- `certificate_generator.py`
- `ai_usage_explainer.py`

**Implementation Notes:**
```
IF max_detection - min_detection > 50:
    detection_status = "INCONCLUSIVE"
    suppress_attribution = True
    suppress_section_breakdown = True
```

---

### 2. Fix Trust Score / NIST Compliance Contradiction

**Problem:** Trust score 53.0/100 triggers escalation requirements, but NIST shows 96.2% "Excellent Compliance". These cannot both be true.

**Solution:**
- Create dependency rules between trust score and NIST compliance
- IF `trust_score < 70`, cap `NIST_GOVERN_score` at 75
- IF any pillar score < 70, change label from "EXCELLENT" to "MODERATE"

**Files to Modify:**
- `nist_compliance_checker.py`
- `trust_score_calculator.py`

**Implementation Notes:**
```python
def apply_trust_nist_alignment(trust_score, nist_scores):
    if trust_score < 70:
        nist_scores['GOVERN']['score'] = min(nist_scores['GOVERN']['score'], 75)
        nist_scores['GOVERN']['label'] = 'MODERATE' if nist_scores['GOVERN']['label'] == 'EXCELLENT' else nist_scores['GOVERN']['label']
    return nist_scores
```

---

### 3. Fix Fairness Failure Impact on NIST MEASURE Score

**Problem:** 33.3% fairness score (FAILED) cannot logically coexist with 100% NIST MEASURE compliance. MEASURE function explicitly requires fairness assessment.

**Solution:**
- IF `fairness_score < 40`, cap `NIST_MEASURE_score` at 60
- Add cross-validation between ethical framework and NIST pillars

**Files to Modify:**
- `nist_compliance_checker.py`
- `bias_auditor.py`

**Implementation Notes:**
```python
def apply_fairness_measure_constraint(fairness_score, nist_measure_score):
    if fairness_score < 40:
        return min(nist_measure_score, 60)
    return nist_measure_score
```

---

## ðŸŸ  HIGH PRIORITY

### 4. Revise Domain Adjustment Formula for Legislation

**Problem:** Current -30% AI detection adjustment is arbitrary and insufficient. Legislative documents have inherent patterns (enumeration, structured lists, stakeholder focus) that trigger false positives.

**Solution Options:**
- **Option A:** Increase adjustment to -50% for legislative documents
- **Option B:** Implement whitelist excluding specific patterns:
  - "structured lists"
  - "stakeholder focus"
  - "enumeration patterns"
  - "section references"
- Publish calculation formula for transparency

**Files to Modify:**
- `document_type_baselines.py`
- `legislative_baseline.py` (if exists)
- `deep_analyzer.py`

**Implementation Notes:**
```python
LEGISLATIVE_PATTERN_WHITELIST = [
    'structured_enumeration',
    'stakeholder_references', 
    'section_numbering',
    'dollar_amount_formatting',
    'date_references'
]
```

---

### 5. Fix Classification Logic Paradox

**Problem:** Document with 92.1% Policy Consequentiality ("Transformative") is labeled "Questionable Policy". This is a logical contradiction that destroys user trust.

**Solution:**
- Revise classification rules:
  - "Questionable" requires `composite < 60` OR `3+ failing categories`
  - IF `PC >= 90` AND `composite >= 70`, use "Transformative Policy (with limitations)"
- Create classification matrix documentation

**Files to Modify:**
- `sparrow_grader_v8.py`

**Implementation Notes:**
```python
def determine_classification(composite_score, pc_score, failing_categories):
    if pc_score >= 90 and composite_score >= 70:
        return "Transformative Policy (with limitations)" if failing_categories > 0 else "Transformative Policy"
    if composite_score < 60 or failing_categories >= 3:
        return "Questionable Policy"
    # ... existing logic
```

---

### 6. Redesign Certificate Visual Hierarchy

**Problem:** Detection uncertainty warning is buried at bottom of certificate. Critical failures don't stand out visually. Users may miss important warnings.

**Solution:**
- Move detection uncertainty warning to TOP of certificate
- Create "Critical Findings" section at top
- Make failing scores larger/more prominent than passing scores
- Add "X of Y critical thresholds met" summary box

**Files to Modify:**
- `certificate_generator.py`

**Implementation Notes:**
```
Certificate Structure:
1. CRITICAL FINDINGS (if any) - RED BOX AT TOP
2. Detection Uncertainty Warning (if applicable)
3. X of Y Critical Thresholds Met
4. Summary Scores
5. Detailed Analysis
6. Methodology Notes
```

---

## ðŸŸ¡ MEDIUM PRIORITY

### 7. Exclude Legislative Operative Language from Data Lineage

**Problem:** Legislative dollar amounts, percentages, and section references are provisions being enacted, not empirical claims requiring sources. Reporting "0% sourcing" is misleading.

**Solution:**
- Distinguish "empirical claims" from "legislative provisions"
- Exclude operative legislative language from claim extraction
- Add document-type aware claim filtering

**Files to Modify:**
- `data_lineage_source_mapper.py`
- `citation_quality_scorer.py`

**Implementation Notes:**
```python
LEGISLATIVE_PROVISION_PATTERNS = [
    r'\$[\d,]+(?:\.\d{2})?',  # Dollar amounts
    r'section \d+',           # Section references
    r'subsection \(\d+\)',    # Subsection references
    r'Part [IVX]+',           # Part references
]
```

---

### 8. Harmonize All AI Percentage Reporting

**Problem:** Reports show conflicting percentages in different places:
- 11.8% (one location)
- 18.0% (another location)
- 46.9% (yet another)

This confusion undermines credibility.

**Solution:**
- Select ONE authoritative figure OR acknowledge INCONCLUSIVE
- Remove dual-score reporting
- If spread >50%, report "INCONCLUSIVE" not averaged score

**Files to Modify:**
- `ai_usage_explainer.py`
- `certificate_generator.py`
- `narrative_engine.py`

---

### 9. Suppress Attribution Confidence When Inconclusive

**Problem:** Certificate shows "90% Cohere confidence" when we don't even know if AI was used. This is statistically meaningless.

**Solution:**
- When detection spread >50%, report attribution as "N/A - Detection inconclusive"
- Do NOT show specific model attribution percentages

**Files to Modify:**
- `certificate_generator.py`
- `ai_disclosure_generator.py`

**Implementation Notes:**
```python
if detection_spread > 50:
    attribution_display = "N/A - Detection inconclusive"
    model_confidence = None
```

---

### 10. Add Confidence Intervals to Probabilistic Scores

**Problem:** Scores are reported as single point estimates without uncertainty bounds. Users interpret "18%" as precise when it's actually "18% Â± 35%".

**Solution:**
- All probabilistic scores should show confidence intervals
- Format: "AI Content: 18% Â± 35%" or "Range: 5%-90%"
- Calculate intervals from method agreement/disagreement

**Files to Modify:**
- `deep_analyzer.py`
- `certificate_generator.py`

---

## ðŸŸ¢ LOW PRIORITY

### 11. Fix Provenance Analysis Stage Status

**Problem:** Stage 3 shows "Pending" while all other stages show "Completed". Unclear if intentional or bug.

**Solution:**
- Clarify if "Pending" is intentional (feature not implemented)
- If intentional, add explanation in output
- If bug, implement the stage or mark as "N/A"

**Files to Modify:**
- `data_lineage_visualizer.py`
- `sparrow_grader_v8.py`

---

### 12. Standardize Narrative Generation Voice

**Problem:** Ollama summary uses "pretty good" (colloquial) which is inconsistent with formal analytical tone elsewhere.

**Solution:**
- Standardize to formal analytical voice
- Verify social media character counts (<280 for Twitter/X)
- Clean up JSON schema redundancies

**Files to Modify:**
- `ollama_summary_generator.py`
- `ai_disclosure_generator.py`

---

## Files Affected Summary

| File | Todo Items |
|------|------------|
| `ai_detection_engine.py` | 1 |
| `nist_compliance_checker.py` | 2, 3 |
| `trust_score_calculator.py` | 2 |
| `bias_auditor.py` | 3 |
| `certificate_generator.py` | 1, 6, 8, 9, 10 |
| `ai_usage_explainer.py` | 1, 8 |
| `sparrow_grader_v8.py` | 5, 11 |
| `document_type_baselines.py` | 4 |
| `deep_analyzer.py` | 4, 10 |
| `data_lineage_source_mapper.py` | 7 |
| `citation_quality_scorer.py` | 7 |
| `narrative_engine.py` | 8 |
| `ai_disclosure_generator.py` | 9, 12 |
| `data_lineage_visualizer.py` | 11 |
| `ollama_summary_generator.py` | 12 |

---

## Estimated Effort

| Priority | Items | Est. Hours |
|----------|-------|------------|
| ðŸ”´ Immediate | 3 | 6-8 hrs |
| ðŸŸ  High | 3 | 8-10 hrs |
| ðŸŸ¡ Medium | 4 | 6-8 hrs |
| ðŸŸ¢ Low | 2 | 2-3 hrs |
| **Total** | **12** | **22-29 hrs** |

---

## Version Target

**Target Version:** v8.4.0  
**Codename:** "Coherence Release"

---

## Approval

- [ ] Todo items reviewed
- [ ] Priority order approved
- [ ] Implementation authorized

**Reviewed by:** ________________________  
**Date:** ________________________

