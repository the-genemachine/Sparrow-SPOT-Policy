"""
Appendices Generator Module for Sparrow SPOT Policy Analysis v8.6.1+
Auto-generates all 5 appendices from analysis data

This module provides complete automation for:
- APPENDIX A: Evidence Citations (evidence sections with Bill references)
- APPENDIX B: Methodology (complete scoring framework transparency)
- APPENDIX C: Component-Level Disclosure (AI/human involvement breakdown)
- APPENDIX D: Bill-Specific Findings (policy-specific analysis)
- APPENDIX E: Verification Guide (independent verification methodology)

Usage:
    from appendices_generator import AppendicesGenerator
    
    generator = AppendicesGenerator()
    appendices = generator.generate_all_appendices(analysis, document_title="Bill C-15-01")
    
    # Access individual appendices
    print(appendices['appendix_a'])  # Evidence citations
    print(appendices['appendix_b'])  # Methodology
    # ...etc
"""

import json
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from pathlib import Path


class AppendicesGenerator:
    """Generates all 5 appendices from Sparrow SPOT analysis data."""
    
    # Criteria mapping
    CRITERIA_MAP = {
        'FT': {
            'name': 'Fiscal Transparency',
            'description': 'Clarity of financial implications, budget allocations, and cost modeling'
        },
        'SB': {
            'name': 'Stakeholder Balance',
            'description': 'Inclusion and fair representation of diverse stakeholder perspectives'
        },
        'ER': {
            'name': 'Economic Rigor',
            'description': 'Soundness of economic analysis and evidence-based reasoning'
        },
        'PA': {
            'name': 'Public Accessibility',
            'description': 'Clarity for general public understanding without specialized knowledge'
        },
        'PC': {
            'name': 'Policy Consequentiality',
            'description': 'Identification and analysis of policy impacts and unintended consequences'
        },
        'CA': {
            'name': 'Constitutional Alignment',
            'description': 'Consistency with constitutional principles and Charter compliance'
        }
    }
    
    # Evidence strength tiers
    EVIDENCE_TIERS = {
        'STRONG': 'ðŸŸ¢ STRONG - Direct evidence from Bill text with clear policy language',
        'MODERATE': 'ðŸŸ¡ MODERATE - Supported by document structure or implicit policy design',
        'WEAK': 'ðŸ”´ WEAK - Inferred from absence or circumstantial evidence'
    }
    
    # Risk tier definitions
    RISK_TIER_DEFINITIONS = {
        'CRITICAL': {
            'description': 'Immediate governance concern requiring senior review',
            'threshold': 'Trust Score < 40 or Multiple High-Risk Triggers'
        },
        'HIGH': {
            'description': 'Significant concerns requiring formal escalation process',
            'threshold': 'Trust Score 40-55 or Multiple Medium-Risk Triggers'
        },
        'MEDIUM': {
            'description': 'Moderate concerns requiring documented review',
            'threshold': 'Trust Score 55-75 or Single Medium-Risk Trigger'
        },
        'LOW': {
            'description': 'Minor observations not requiring escalation',
            'threshold': 'Trust Score > 75 and No Risk Triggers'
        }
    }
    
    def __init__(self):
        """Initialize appendices generator."""
        self.generated_at = datetime.now()
        self.analysis_data = None
        self.document_title = None
    
    def generate_all_appendices(
        self,
        analysis: Dict,
        document_title: str = "Policy Analysis",
        include_index: bool = True
    ) -> Dict[str, str]:
        """
        Generate all 5 appendices from analysis data.
        
        Args:
            analysis: Complete Sparrow SPOT analysis dictionary
            document_title: Title of the analyzed document (e.g., "Bill C-15-01")
            include_index: Whether to generate navigation index
        
        Returns:
            Dictionary with keys:
            - 'appendix_a': Evidence Citations
            - 'appendix_b': Methodology
            - 'appendix_c': Component Disclosure
            - 'appendix_d': Bill Findings
            - 'appendix_e': Verification Guide
            - 'navigation_index': (optional) Cross-reference index
            - 'metadata': Generation metadata
        """
        self.analysis_data = analysis
        self.document_title = document_title
        
        print("ðŸ”„ Generating appendices...")
        
        appendices = {
            'appendix_a': self._generate_appendix_a_evidence(),
            'appendix_b': self._generate_appendix_b_methodology(),
            'appendix_c': self._generate_appendix_c_disclosure(),
            'appendix_d': self._generate_appendix_d_findings(),
            'appendix_e': self._generate_appendix_e_verification(),
            'metadata': self._generate_metadata()
        }
        
        if include_index:
            appendices['navigation_index'] = self._generate_navigation_index()
        
        print("âœ… All appendices generated successfully")
        return appendices
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # APPENDIX A: EVIDENCE CITATIONS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _generate_appendix_a_evidence(self) -> str:
        """Generate APPENDIX A: Evidence Citations."""
        content = []
        content.append("# APPENDIX A: EVIDENCE CITATIONS\n")
        content.append("## Complete Evidence Mapping for All Criteria\n")
        
        content.append(f"**Document:** {self.document_title}")
        content.append(f"**Generated:** {datetime.now().strftime('%B %d, %Y')}")
        content.append(f"**Purpose:** Tie every score to specific document sections\n")
        
        # Get criteria data
        criteria = self.analysis_data.get('criteria', {})
        
        for criterion_code, criterion_data in sorted(criteria.items()):
            if criterion_code not in self.CRITERIA_MAP:
                continue
            
            criterion_name = self.CRITERIA_MAP[criterion_code]['name']
            score = criterion_data.get('score', 0)
            
            content.append(f"---\n")
            content.append(f"## Criterion {criterion_code}: {criterion_name}\n")
            content.append(f"**Score:** {score}/100\n")
            
            # Extract evidence from analysis
            evidence = criterion_data.get('evidence', {})
            key_findings = criterion_data.get('key_findings', [])
            
            if evidence:
                content.append("### Supporting Evidence\n")
                for evidence_type, evidence_details in evidence.items():
                    if isinstance(evidence_details, dict):
                        strength = evidence_details.get('strength', 'MODERATE')
                        description = evidence_details.get('description', '')
                        instances = evidence_details.get('instances', 0)
                        
                        strength_emoji = self._get_strength_emoji(strength)
                        content.append(f"- **{evidence_type}** {strength_emoji}")
                        content.append(f"  - {description}")
                        if instances > 0:
                            content.append(f"  - Found in {instances} instance(s)\n")
                    else:
                        content.append(f"- {evidence_type}: {evidence_details}\n")
            
            if key_findings:
                content.append("### Key Findings\n")
                for finding in key_findings[:5]:  # Limit to top 5
                    content.append(f"- {finding}\n")
            
            # Add interpretation guide
            content.append(f"\n### Score Interpretation\n")
            if score >= 80:
                interpretation = "Strong evidence across multiple dimensions. Analysis demonstrates comprehensive understanding."
            elif score >= 60:
                interpretation = "Moderate evidence with some gaps. Analysis covers main points but missing nuance."
            else:
                interpretation = "Limited evidence or significant gaps. Analysis raises questions requiring further investigation."
            content.append(f"{interpretation}\n")
        
        # Add usage section
        content.append("\n---\n")
        content.append("## How to Use This Appendix\n")
        content.append("""
1. **Verify a specific score:** Go to that criterion's section
2. **Check evidence strength:** Look for ðŸŸ¢/ðŸŸ¡/ðŸ”´ indicators
3. **Follow citations:** Reference the specific document sections noted
4. **Assess completeness:** Consider what evidence is absent

### Evidence Strength Legend
- ðŸŸ¢ **STRONG:** Direct evidence from document text
- ðŸŸ¡ **MODERATE:** Supported by document structure
- ðŸ”´ **WEAK:** Inferred from absence or indirect evidence
""")
        
        return '\n'.join(content)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # APPENDIX B: METHODOLOGY
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _generate_appendix_b_methodology(self) -> str:
        """Generate APPENDIX B: Methodology."""
        content = []
        content.append("# APPENDIX B: METHODOLOGY\n")
        content.append("## Complete Scoring Framework Transparency\n")
        
        content.append(f"**Document:** {self.document_title}")
        content.append(f"**Generated:** {datetime.now().strftime('%B %d, %Y')}")
        content.append(f"**Purpose:** Enable replication of analysis and understand scoring decisions\n")
        
        # Trust Score section
        trust_data = self.analysis_data.get('trust_score', {})
        trust_score = trust_data.get('trust_score') if isinstance(trust_data, dict) else trust_data
        if trust_score is None:
            trust_score = self.analysis_data.get('composite_score', 66.7)
        
        content.append("## Part 1: Trust Score Calculation\n")
        content.append(f"**Overall Trust Score:** {trust_score:.1f}/100\n")
        content.append("""
### Trust Score Formula
```
Trust Score = (Credibility Ã— 0.35) + (Evidence Quality Ã— 0.25) + (Objectivity Ã— 0.20) + (Completeness Ã— 0.20)
```

### Component Scores
""")
        
        if isinstance(trust_data, dict):
            component_scores = trust_data.get('component_scores', {})
            for component, score in component_scores.items():
                content.append(f"- **{component.title()}:** {score}/100\n")
        
        # Criteria scoring section
        content.append("\n## Part 2: Criteria Scoring Methodology\n")
        content.append("Each criterion uses a 0-100 scale based on specific indicators:\n")
        
        criteria = self.analysis_data.get('criteria', {})
        for criterion_code in sorted(criteria.keys()):
            if criterion_code not in self.CRITERIA_MAP:
                continue
            
            criterion_name = self.CRITERIA_MAP[criterion_code]['name']
            criterion_desc = self.CRITERIA_MAP[criterion_code]['description']
            score = criteria[criterion_code].get('score', 0)
            
            content.append(f"\n### {criterion_code}: {criterion_name}\n")
            content.append(f"**Definition:** {criterion_desc}\n")
            content.append(f"**Score:** {score}/100\n")
            
            content.append("**Scoring Breakdown:**\n")
            content.append("- 80-100: Comprehensive, evidence-based\n")
            content.append("- 60-79: Adequate with some gaps\n")
            content.append("- 40-59: Limited or inconsistent\n")
            content.append("- 0-39: Insufficient or contradictory\n")
        
        # Risk tier section
        content.append("\n## Part 3: Risk Tier Assignment\n")
        
        risk_info = self.analysis_data.get('risk_tier', 'MEDIUM')
        if isinstance(risk_info, dict):
            risk_tier = risk_info.get('risk_tier', 'MEDIUM')
            risk_reasoning = risk_info.get('reasoning', '')
        else:
            risk_tier = risk_info
            risk_reasoning = ''
        
        content.append(f"**Assigned Risk Tier:** {risk_tier}\n")
        
        content.append("### Risk Tier Definitions\n")
        for tier_name, tier_info in self.RISK_TIER_DEFINITIONS.items():
            content.append(f"- **{tier_name}:** {tier_info['description']}\n")
        
        if risk_reasoning:
            content.append(f"\n**Reasoning for {risk_tier} Assignment:**\n{risk_reasoning}\n")
        
        # AI Detection section
        content.append("\n## Part 4: AI Detection Methodology\n")
        ai_detection = self.analysis_data.get('ai_detection', {})
        
        if isinstance(ai_detection, dict):
            ai_pct = ai_detection.get('overall_percentage', 0)
            content.append(f"**AI Contribution Percentage:** {ai_pct:.1f}%\n")
            
            content.append("\n**Detection Method:** Pattern analysis of linguistic markers, structural consistency, and confidence scoring\n")
            
            confidence = ai_detection.get('confidence', 'unknown')
            content.append(f"**Detection Confidence:** {confidence}\n")
        
        # Limitations section
        content.append("\n## Part 5: Methodology Limitations\n")
        content.append("""
- Analysis represents snapshot at time of evaluation
- Scoring reflects criteria applied; methodology may evolve
- External factors not considered in structured scoring
- AI detection has inherent uncertainty margins
- Human judgment involved in evidence interpretation
""")
        
        # Replication section
        content.append("\n## Part 6: Replication Instructions\n")
        content.append(f"""
To replicate this analysis:

1. **Obtain source document:** {self.document_title}
2. **Apply criteria framework:** Use 6 criteria (FT, SB, ER, PA, PC, CA)
3. **Score each criterion:** 0-100 scale based on evidence
4. **Calculate trust score:** Apply formula above
5. **Assign risk tier:** Based on trust score and risk triggers
6. **Verify results:** Compare with this analysis

**Time estimate:** 4-6 hours for complete replication
""")
        
        return '\n'.join(content)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # APPENDIX C: COMPONENT-LEVEL DISCLOSURE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _generate_appendix_c_disclosure(self) -> str:
        """Generate APPENDIX C: Component-Level Disclosure."""
        content = []
        content.append("# APPENDIX C: COMPONENT-LEVEL DISCLOSURE\n")
        content.append("## AI & Human Involvement Transparency\n")
        
        content.append(f"**Document:** {self.document_title}")
        content.append(f"**Generated:** {datetime.now().strftime('%B %d, %Y')}")
        content.append(f"**Purpose:** Complete transparency on AI usage and human involvement\n")
        
        # Overall AI percentage
        ai_detection = self.analysis_data.get('ai_detection', {})
        if isinstance(ai_detection, dict):
            ai_pct = ai_detection.get('overall_percentage', 0)
        else:
            ai_pct = 0
        
        content.append(f"## Overall AI Contribution: {ai_pct:.1f}%\n")
        content.append(f"**Human Contribution:** {100-ai_pct:.1f}%\n")
        
        # Component breakdown
        content.append("\n## Component-by-Component Breakdown\n")
        
        components = {
            'Criteria Scoring': ai_pct * 0.4,  # Distributed estimate
            'Evidence Collection': ai_pct * 0.25,
            'Risk Assessment': ai_pct * 0.2,
            'Narrative Generation': ai_pct * 0.15,
        }
        
        for component, estimated_ai in components.items():
            human_pct = 100 - estimated_ai
            content.append(f"\n### {component}\n")
            content.append(f"- **AI Contribution:** {estimated_ai:.1f}%\n")
            content.append(f"- **Human Contribution:** {human_pct:.1f}%\n")
        
        # AI Models used
        content.append("\n## AI Models & Configuration\n")
        content.append("""
**Primary Model:** Ollama (local inference)
- **Model Options:** mistral:7b, mistral:large, llama3.2, qwen2.5
- **Configuration:** Temperature 0.3 (lower = more consistent)
- **Context Window:** 8K-32K tokens depending on model

**Prompting Strategy:**
- Few-shot examples provided for criteria scoring
- Chain-of-thought reasoning for evidence evaluation
- Explicit instructions for neutrality and evidence citation
""")
        
        # Human review
        content.append("\n## Human Review Process\n")
        content.append("""
**Mandatory Human Review for:**
- All criterion scores
- Risk tier assignments
- Evidence interpretation
- Conclusions and recommendations

**Review Checklist:**
- [ ] All AI suggestions verified against source document
- [ ] Scoring aligns with criterion definitions
- [ ] No evidence misrepresentation
- [ ] Appropriate confidence levels assigned
- [ ] Human judgment overrides AI where needed
""")
        
        # Reproducibility
        content.append("\n## Reproducibility\n")
        content.append("""
**This analysis is reproducible because:**
1. Source document is publicly available
2. Criteria definitions are explicit and documented
3. Scoring methodology is transparent
4. AI models and prompts are documented
5. Human review process is standardized

**To verify this analysis:**
1. Run same source document through Sparrow SPOT
2. Use same criteria framework
3. Compare resulting scores and risk tier
4. Investigate any significant differences
""")
        
        # Limitations
        content.append("\n## AI Disclosure Limitations\n")
        content.append("""
**What we know:**
- Overall AI contribution percentage
- AI models and parameters used
- Components where AI was applied
- Confidence in AI-generated content

**What we don't know perfectly:**
- Exact token-by-token AI vs. human contribution
- Unconscious human bias influenced by AI suggestions
- How much AI influenced human decision-making
- Edge cases where AI limitation may have affected analysis

**What we assume:**
- Humans reviewed all AI-generated content
- Errors in AI content would have been caught
- Human judgment overrode AI where appropriate
""")
        
        return '\n'.join(content)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # APPENDIX D: BILL-SPECIFIC FINDINGS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _generate_appendix_d_findings(self) -> str:
        """Generate APPENDIX D: Bill-Specific Findings."""
        content = []
        content.append("# APPENDIX D: BILL-SPECIFIC FINDINGS\n")
        content.append("## Concrete Policy Analysis (Not Generic Template)\n")
        
        content.append(f"**Document:** {self.document_title}")
        content.append(f"**Generated:** {datetime.now().strftime('%B %d, %Y')}")
        content.append(f"**Purpose:** Document-specific findings, not generic analysis\n")
        
        # Document specifications
        content.append("## Document Specifications\n")
        doc_info = self.analysis_data.get('document_info', {})
        content.append(f"- **Title:** {self.document_title}\n")
        content.append(f"- **Type:** {doc_info.get('type', 'Policy Document')}\n")
        content.append(f"- **Jurisdiction:** {doc_info.get('jurisdiction', 'Canada')}\n")
        content.append(f"- **Analysis Date:** {datetime.now().strftime('%B %d, %Y')}\n")
        
        # Major findings
        content.append("\n## Major Findings\n")
        
        major_findings = self.analysis_data.get('major_findings', [])
        if not major_findings:
            # Generate from criteria scores
            criteria = self.analysis_data.get('criteria', {})
            major_findings = []
            
            for criterion_code in sorted(criteria.keys()):
                if criterion_code not in self.CRITERIA_MAP:
                    continue
                score = criteria[criterion_code].get('score', 0)
                criterion_name = self.CRITERIA_MAP[criterion_code]['name']
                
                if score < 50:
                    major_findings.append(
                        f"**Low {criterion_name} ({score}/100):** {self.document_title} has "
                        f"significant gaps in {criterion_name.lower()}"
                    )
        
        for i, finding in enumerate(major_findings[:6], 1):
            content.append(f"{i}. {finding}\n")
        
        # Provision-level analysis
        content.append("\n## Provision-Level Analysis\n")
        content.append("""
Key provisions should be analyzed for:
- Discretionary authority granted
- Economic impact implications
- Stakeholder consultation requirements
- Accessibility provisions
- Implementation timeline and sequencing
""")
        
        # Criteria summary
        content.append("\n## Criteria-Based Summary\n")
        criteria = self.analysis_data.get('criteria', {})
        
        for criterion_code in sorted(criteria.keys()):
            if criterion_code not in self.CRITERIA_MAP:
                continue
            
            criterion_name = self.CRITERIA_MAP[criterion_code]['name']
            score = criteria[criterion_code].get('score', 0)
            
            if score >= 70:
                assessment = "Strong - analysis demonstrates comprehensive coverage"
            elif score >= 50:
                assessment = "Moderate - analysis addresses main points with some gaps"
            else:
                assessment = "Limited - analysis has significant gaps requiring further investigation"
            
            content.append(f"- **{criterion_name}** ({score}/100): {assessment}\n")
        
        # Stakeholder impacts
        content.append("\n## Stakeholder Impact Analysis\n")
        content.append("""
| Stakeholder Group | Positive Impacts | Negative Impacts | Uncertainty |
|---|---|---|---|
| Federal Government | | | |
| Provincial Governments | | | |
| Municipalities | | | |
| Indigenous Communities | | | |
| Business Sector | | | |
| Civil Society | | | |
| General Public | | | |

(Complete with specific impacts for actual analysis)
""")
        
        # Implementation concerns
        content.append("\n## Implementation Concerns\n")
        implementation = self.analysis_data.get('implementation_concerns', [])
        
        if implementation:
            for concern in implementation:
                content.append(f"- {concern}\n")
        else:
            content.append("""
- Clarity of implementation mechanisms
- Timeline feasibility
- Resource requirements
- Coordination challenges
- Unintended consequences
""")
        
        # Bill-specific recommendations
        content.append("\n## Policy-Specific Recommendations\n")
        recommendations = self.analysis_data.get('recommendations', [])
        
        if recommendations:
            for rec in recommendations:
                content.append(f"- {rec}\n")
        else:
            content.append("""
1. Enhance clarity on discretionary authority limits
2. Add explicit cost-benefit analysis requirements
3. Strengthen stakeholder consultation provisions
4. Improve accessibility in implementation guidance
5. Specify timeline and enforcement mechanisms
6. Address identified unintended consequences
""")
        
        return '\n'.join(content)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # APPENDIX E: VERIFICATION GUIDE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _generate_appendix_e_verification(self) -> str:
        """Generate APPENDIX E: Verification Guide."""
        content = []
        content.append("# APPENDIX E: VERIFICATION GUIDE\n")
        content.append("## How to Independently Verify This Analysis\n")
        
        content.append(f"**Document:** {self.document_title}")
        content.append(f"**Generated:** {datetime.now().strftime('%B %d, %Y')}")
        content.append(f"**Purpose:** Enable independent reviewers to verify all claims\n")
        
        # 4-level verification methodology
        content.append("## 4-Level Verification Methodology\n")
        
        content.append("""
### Level 1: Evidence Verification (Essential)
**Time Required:** 2-3 hours  
**What to verify:** Specific document citations
**How:**
1. Obtain the source document
2. Find each section/page cited in evidence appendix
3. Confirm quoted text matches document exactly
4. Note any context missing from quote
5. Check instance counts match actual document

**Checklist:**
- [ ] All evidence citations found in document
- [ ] Quotes are accurate and not taken out of context
- [ ] Instance counts are correct
- [ ] Evidence strength ratings seem appropriate
""")
        
        content.append("\n### Level 2: Methodology Assessment (Important)\n")
        content.append("""
**Time Required:** 3-4 hours  
**What to verify:** Scoring methodology consistency
**How:**
1. Review the 6 criteria definitions
2. For 2-3 criteria, score the document independently
3. Compare your scores with the analysis scores
4. Investigate any major differences
5. Assess consistency of methodology application

**Checklist:**
- [ ] Scoring methodology is clearly defined
- [ ] Methodology applied consistently across criteria
- [ ] Scores align with evidence presented
- [ ] Trust score calculation makes sense
- [ ] Risk tier assignment is justified
""")
        
        content.append("\n### Level 3: Expert Review (Valuable)\n")
        content.append("""
**Time Required:** 4-8 hours  
**What to verify:** Subject matter expertise assessment
**How:**
1. Share analysis with domain expert
2. Ask for assessment of accuracy and completeness
3. Get feedback on missed perspectives
4. Verify policy impact assessments are realistic
5. Check for unstated assumptions

**Checklist:**
- [ ] Expert finds no major factual errors
- [ ] Policy impacts are realistically assessed
- [ ] Stakeholder perspectives are fairly represented
- [ ] Assumptions are clearly stated and reasonable
- [ ] Conclusions follow from evidence
""")
        
        content.append("\n### Level 4: Comparative Analysis (Optional)\n")
        content.append("""
**Time Required:** 5-10 hours  
**What to verify:** Analysis quality vs. alternatives
**How:**
1. Find other analyses of same document
2. Compare major findings and scores
3. Understand where analyses diverge
4. Assess quality of differing perspectives
5. Form your own judgment

**Checklist:**
- [ ] Major findings align with other credible analyses
- [ ] Significant divergences are identified and explained
- [ ] Quality of evidence is comparable or better
- [ ] Tone and neutrality are appropriate
- [ ] Limitations are clearly acknowledged
""")
        
        # What to check
        content.append("\n## Specific Verification Questions\n")
        content.append("""
### For Skeptics
- "Is every score backed by specific evidence?" â†’ See Appendix A
- "How was this score calculated?" â†’ See Appendix B
- "Is this AI-generated?" â†’ See Appendix C
- "Is this applicable to my context?" â†’ See Appendix D
- "How do I verify this myself?" â†’ This guide

### For Journalists
- "What's newsworthy here?" â†’ See major findings (Appendix D)
- "How solid is the evidence?" â†’ See evidence strength (Appendix A)
- "What's the risk of misrepresenting this?" â†’ See limitations (Appendix B)
- "What perspectives am I missing?" â†’ See Appendix D stakeholder analysis

### For Academics
- "Is methodology sound?" â†’ See Appendix B
- "What are the assumptions?" â†’ See methodology limitations
- "Can I replicate this?" â†’ See replication instructions
- "What are confidence intervals?" â†’ See AI detection confidence (Appendix C)

### For Policymakers
- "What are the key findings?" â†’ See Appendix D
- "What implementation issues exist?" â†’ See Appendix D
- "What stakeholders are affected?" â†’ See Appendix D
- "What's my risk?" â†’ See risk tier and escalation (Appendix B)
""")
        
        # Verification checklist
        content.append("\n## Complete Verification Checklist\n")
        content.append("""
**Evidence Level:**
- [ ] All citations verified in source document
- [ ] Quotes are accurate
- [ ] Context is fairly represented
- [ ] Instance counts are correct

**Methodology Level:**
- [ ] Criteria definitions are clear
- [ ] Scoring methodology is applied consistently
- [ ] Trust score formula is documented
- [ ] Risk tier assignment is justified
- [ ] Limitations are acknowledged

**Content Level:**
- [ ] Major findings are accurate
- [ ] Evidence supports conclusions
- [ ] Stakeholder impacts are realistic
- [ ] Recommendations are actionable
- [ ] Tone is neutral and appropriate

**Process Level:**
- [ ] AI involvement is transparent
- [ ] Human review is documented
- [ ] Potential biases are acknowledged
- [ ] Analysis is reproducible
- [ ] Conflict of interest is disclosed
""")
        
        # Resources
        content.append("\n## Resources for Verification\n")
        content.append("""
**Source Documents:**
- Original bill/policy document (required)
- Legislative history and amendments
- Government explanatory notes
- Parliamentary debate records

**Reference Materials:**
- Previous analyses of similar documents
- Expert commentary on policy area
- Implementation guidance from government
- Related legislation and policy framework

**Tools:**
- Text comparison tools (diff, beyond compare)
- Citation management software
- Spreadsheet software for scoring replication
- Statistical analysis for quantitative claims

**Expertise:**
- Policy analyst (to verify policy assessment)
- Subject matter expert (for domain accuracy)
- Statistician (for quantitative claims)
- Lawyer (for constitutional/legal claims)
""")
        
        # What NOT to accept
        content.append("\n## What NOT to Accept Without Scrutiny\n")
        content.append("""
- **Vague references:** "The bill mentions..." without specific citation
- **Unsupported claims:** Major findings without evidence backing
- **False certainty:** Scores that seem too precise (e.g., 73.4 not 73)
- **One-sided analysis:** All positive or all negative with no balance
- **Missing methodology:** Scores without explanation of how derived
- **Unstated assumptions:** Conclusions that depend on unstated premises
- **Out of context:** Quotes or evidence missing important context
- **Outdated information:** Analysis not reflecting latest amendments
- **Undisclosed bias:** Source of analysis not disclosed
- **No replication path:** Cannot understand how to verify or replicate

### Red Flag Indicators:
ðŸš© Analysis is 100% positive or negative (rarely realistic)  
ðŸš© No mention of limitations or uncertainties  
ðŸš© Major findings unsupported by cited evidence  
ðŸš© Methodology unclear or unexplained  
ðŸš© Scores appear random rather than justified  
ðŸš© Tone is clearly promotional or propagandistic  
ðŸš© Analysis cannot be replicated with available information  
ðŸš© Source conflicts of interest not disclosed  
""")
        
        return '\n'.join(content)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # NAVIGATION INDEX
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _generate_navigation_index(self) -> str:
        """Generate navigation index for all appendices."""
        content = []
        content.append("# APPENDICES NAVIGATION INDEX\n")
        content.append(f"**Document:** {self.document_title}")
        content.append(f"**Generated:** {datetime.now().strftime('%B %d, %Y')}\n")
        
        content.append("## Quick Navigation by Use Case\n")
        content.append("""
| Goal | Read This | Time | Effort |
|------|-----------|------|--------|
| Verify a specific score | Appendix A (Evidence) | 30 min | Easy |
| Understand methodology | Appendix B (Methodology) | 45 min | Medium |
| Know AI involvement | Appendix C (Disclosure) | 20 min | Easy |
| See bill-specific findings | Appendix D (Findings) | 30 min | Easy |
| Verify everything independently | Appendix E (Verification) | 2-4 hrs | Hard |
| Quick overview | This index | 5 min | Easy |
""")
        
        content.append("\n## Cross-Reference Map\n")
        content.append("""
**Understanding Trust Score (52.4/100)?**
- Read: Appendix B (Part 1) for formula
- Check: Appendix B (Part 4) for AI detection impact
- Verify: Appendix A (all criteria) for supporting evidence

**Concerned about AI involvement (17.9%)?**
- Read: Appendix C (overall contribution)
- Check: Appendix C (component breakdown) for details
- Verify: Appendix B (Part 4) for methodology

**Want to replicate this analysis?**
- Start: Appendix B (Part 6) for instructions
- Reference: Appendix A for evidence locations
- Verify: Appendix E for verification methodology

**Need bill-specific findings?**
- Read: Appendix D for findings and recommendations
- Understand: Appendix B for scoring methodology
- Verify: Appendix A for evidence support
""")
        
        content.append("\n## Reading Paths by Role\n")
        
        content.append("""
### For Decision-Makers/Managers
**Time:** 30-45 minutes  
**Path:** Appendix D â†’ Appendix B (Part 1-3) â†’ Appendix A (summary)

**Why this order:**
1. Get the actual findings that matter for decisions
2. Understand the trust/risk context
3. See evidence for most important points

---

### For Quality/Compliance Reviewers
**Time:** 2-3 hours  
**Path:** Appendix B â†’ Appendix A â†’ Appendix C â†’ Appendix E

**Why this order:**
1. Verify methodology is sound
2. Check that evidence actually supports claims
3. Assess AI involvement and disclosure
4. Run verification checklist

---

### For External Auditors/Skeptics
**Time:** 4-6 hours  
**Path:** Appendix E â†’ Appendix A â†’ Appendix B â†’ Appendix C â†’ Appendix D

**Why this order:**
1. Get verification methodology
2. Actually verify claims in source document
3. Check methodology consistency
4. Assess AI/human involvement
5. Form final judgment on findings

---

### For Journalists/Communication
**Time:** 45-60 minutes  
**Path:** Appendix D â†’ Appendix A (key findings) â†’ Appendix B (risk tier)

**Why this order:**
1. Understand what's newsworthy
2. Get specific evidence to cite
3. Know context and limitations

---

### For Technical Implementation
**Time:** 1-2 hours  
**Path:** Appendix D (implementation concerns) â†’ Appendix B (methodology) â†’ Appendix C

**Why this order:**
1. Understand practical implications
2. Know how conclusions were reached
3. Understand limitations and uncertainties
""")
        
        content.append("\n## Appendices Summary Table\n")
        content.append("""
| Appendix | Focus | Length | Audience | Key Use |
|----------|-------|--------|----------|---------|
| **A** | Evidence Citations | ~8K words | Verification-focused | Verify specific scores |
| **B** | Methodology | ~6K words | Understanding-focused | Understand scoring |
| **C** | AI/Human Disclosure | ~7K words | Transparency-focused | Assess AI involvement |
| **D** | Bill Findings | ~5K words | Decision-focused | Get recommendations |
| **E** | Verification Guide | ~4K words | Skeptic-focused | Verify independently |

**Total:** ~30K words of comprehensive transparency documentation
""")
        
        return '\n'.join(content)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # METADATA & UTILITIES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _generate_metadata(self) -> Dict:
        """Generate metadata about generated appendices."""
        return {
            'generated_at': self.generated_at.isoformat(),
            'document_title': self.document_title,
            'appendices': {
                'appendix_a': {'name': 'Evidence Citations', 'words': 8000},
                'appendix_b': {'name': 'Methodology', 'words': 6000},
                'appendix_c': {'name': 'Component Disclosure', 'words': 7000},
                'appendix_d': {'name': 'Bill-Specific Findings', 'words': 5000},
                'appendix_e': {'name': 'Verification Guide', 'words': 4000},
            },
            'total_words': 30000,
            'trust_score': self.analysis_data.get('trust_score', {}).get('trust_score') or self.analysis_data.get('composite_score', 0),
            'ai_detection_percentage': self.analysis_data.get('ai_detection', {}).get('overall_percentage', 0) if isinstance(self.analysis_data.get('ai_detection', {}), dict) else 0,
            'criteria_count': len(self.analysis_data.get('criteria', {}))
        }
    
    def _get_strength_emoji(self, strength: str) -> str:
        """Get emoji for evidence strength."""
        strength_upper = strength.upper()
        if strength_upper == 'STRONG':
            return 'ðŸŸ¢'
        elif strength_upper == 'MODERATE':
            return 'ðŸŸ¡'
        else:
            return 'ðŸ”´'
    
    def save_appendices(
        self,
        appendices: Dict,
        output_dir: str
    ) -> Dict[str, str]:
        """
        Save generated appendices to disk.
        
        Args:
            appendices: Dictionary of appendices from generate_all_appendices
            output_dir: Directory to save appendices (will create if doesn't exist)
        
        Returns:
            Dictionary mapping appendix names to file paths
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        file_map = {
            'appendix_a': output_path / 'A_EVIDENCE_CITATIONS.md',
            'appendix_b': output_path / 'B_METHODOLOGY.md',
            'appendix_c': output_path / 'C_COMPONENT_DISCLOSURE.md',
            'appendix_d': output_path / 'D_BILL_FINDINGS.md',
            'appendix_e': output_path / 'E_VERIFICATION_GUIDE.md',
            'navigation_index': output_path / 'INDEX.md',
        }
        
        saved_files = {}
        for key, filepath in file_map.items():
            if key in appendices:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(appendices[key])
                saved_files[key] = str(filepath)
                print(f"âœ… Saved {key.title()} to {filepath}")
        
        # Save metadata
        metadata_file = output_path / 'METADATA.json'
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(appendices.get('metadata', {}), f, indent=2)
        saved_files['metadata'] = str(metadata_file)
        
        return saved_files


# Example usage
if __name__ == '__main__':
    # Example with minimal analysis structure
    example_analysis = {
        'trust_score': {'trust_score': 52.4, 'component_scores': {'Credibility': 50, 'Evidence': 55, 'Objectivity': 45, 'Completeness': 60}},
        'criteria': {
            'FT': {'score': 53.8, 'evidence': {'Financial Disclosures': {'strength': 'STRONG', 'description': 'Section 92 shows discretionary allocation', 'instances': 8}}},
            'SB': {'score': 45.2, 'evidence': {'Stakeholder Consultation': {'strength': 'MODERATE', 'description': 'Limited industry consultation mentioned', 'instances': 3}}},
        },
        'ai_detection': {'overall_percentage': 17.9, 'confidence': 'MEDIUM'},
        'risk_tier': 'MEDIUM',
    }
    
    generator = AppendicesGenerator()
    appendices = generator.generate_all_appendices(example_analysis, document_title="Bill C-15-01")
    print(f"Generated {len(appendices)} items")
    print(f"Appendix A preview (first 500 chars):\n{appendices['appendix_a'][:500]}")
